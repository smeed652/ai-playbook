# Recipe: Database Migration Strategy

**Category**: workflow
**Version**: 1.0
**Last Updated**: 2025-12-14
**Sprints**: Multiple sprints (migrations created in Sprints 6-116)
**ADRs**: [ADR-010: Alembic Database Migrations](../../architecture/decisions/ADR-010-alembic-database-migrations.md), [ADR-011: SQLAlchemy 2.0 ORM](../../architecture/decisions/ADR-011-sqlalchemy-2-orm.md)

## Context

**When to use this recipe:**
- You need to add new tables or columns to the database schema
- You're modifying existing column types or constraints
- You need to perform data transformations during schema changes
- You're adding database extensions (PostGIS, TimescaleDB)
- You're creating or modifying indexes
- You need reproducible schema changes across environments (dev, staging, production)

**When NOT to use this recipe:**
- For one-time data imports (use scripts in `/scripts/`)
- For temporary testing changes (use `CREATE TEMP TABLE` or test database)
- For rollback-incompatible changes (plan carefully, use maintenance window)
- For changes to external databases you don't control

## Ingredients

Before starting, ensure you have:

- [ ] Alembic installed and configured (`alembic/` directory exists)
- [ ] Database connection working (`DATABASE_URL` environment variable set)
- [ ] SQLAlchemy models updated to reflect desired schema
- [ ] Understanding of the impact of your changes (breaking vs. non-breaking)
- [ ] Access to all environments where migration will run (dev, staging, production)
- [ ] Backup of production database (if applying to production)

## Steps

### Step 1: Update SQLAlchemy Models

First, make changes to your SQLAlchemy models to reflect the desired schema.

```python
# src/corrdata/db/models.py
from sqlalchemy.orm import Mapped, mapped_column
from sqlalchemy import String, Integer, DateTime, ForeignKey
from uuid import UUID

class Asset(Base):
    __tablename__ = "assets"

    uuid: Mapped[UUID] = mapped_column(primary_key=True)
    name: Mapped[str] = mapped_column(String(255))

    # NEW: Add coating type column
    coating_type: Mapped[Optional[str]] = mapped_column(String(50))

    # NEW: Add installation date
    installed_at: Mapped[Optional[datetime]] = mapped_column(DateTime(timezone=True))
```

**Expected outcome**: Models represent the target schema state.

### Step 2: Generate Migration

Use Alembic's autogenerate feature to create a migration from model changes.

```bash
# Generate migration with descriptive message
alembic revision --autogenerate -m "Add coating_type and installed_at to assets"

# Output:
# Generating /path/to/alembic/versions/abc123def456_add_coating_type_and_installed_at_to_assets.py ... done
```

This creates a new migration file in `alembic/versions/`.

**Expected outcome**: New migration file created with auto-detected changes.

### Step 3: Review and Edit Migration

**CRITICAL**: Always review autogenerated migrations. Alembic can't detect everything.

```python
# alembic/versions/abc123def456_add_coating_type_and_installed_at_to_assets.py
"""Add coating_type and installed_at to assets

Revision ID: abc123def456
Revises: previous_revision_id
Create Date: 2025-12-14 10:30:00.000000
"""
from alembic import op
import sqlalchemy as sa

revision = 'abc123def456'
down_revision = 'previous_revision_id'  # Points to previous migration
branch_labels = None
depends_on = None

def upgrade():
    # Add new columns (nullable=True for backward compatibility)
    op.add_column('assets',
        sa.Column('coating_type', sa.String(50), nullable=True)
    )
    op.add_column('assets',
        sa.Column('installed_at', sa.DateTime(timezone=True), nullable=True)
    )

def downgrade():
    # Remove columns (reverse of upgrade)
    op.drop_column('assets', 'installed_at')
    op.drop_column('assets', 'coating_type')
```

**Review checklist:**
- [ ] Column names match model exactly
- [ ] Types are correct (String length, Integer, DateTime with timezone, etc.)
- [ ] Nullable constraints are appropriate
- [ ] Default values are set if needed
- [ ] Downgrade reverses upgrade correctly
- [ ] Foreign keys and indexes are included

**Expected outcome**: Migration accurately reflects intended changes and is reversible.

### Step 4: Test Migration Locally

Apply the migration to your local database and verify it works.

```bash
# Check current migration state
alembic current

# Apply migration
alembic upgrade head

# Verify migration was applied
alembic current
# Should show: abc123def456 (head)

# Test the schema change
psql $DATABASE_URL -c "\d assets"
# Should show new columns: coating_type, installed_at

# Test downgrade (important!)
alembic downgrade -1

# Verify columns removed
psql $DATABASE_URL -c "\d assets"
# Should NOT show coating_type, installed_at

# Re-apply for continued development
alembic upgrade head
```

**Expected outcome**: Migration applies cleanly, downgrade works, schema matches expectations.

### Step 5: Commit Migration to Git

Once tested, commit the migration file.

```bash
git add alembic/versions/abc123def456_add_coating_type_and_installed_at_to_assets.py
git add src/corrdata/db/models.py
git commit -m "feat: add coating_type and installed_at to assets (Sprint 42)"
```

**Expected outcome**: Migration is version controlled and can be applied to other environments.

## Advanced Patterns

### Data Migration with Schema Change

When changing column types or adding non-nullable columns to populated tables:

```python
from alembic import op
from sqlalchemy import text

def upgrade():
    # Step 1: Add new column as nullable
    op.add_column('assets',
        sa.Column('status', sa.String(20), nullable=True)
    )

    # Step 2: Populate with default value
    connection = op.get_bind()
    connection.execute(
        text("UPDATE assets SET status = 'active' WHERE status IS NULL")
    )

    # Step 3: Make non-nullable after data is migrated
    op.alter_column('assets', 'status', nullable=False)

def downgrade():
    op.drop_column('assets', 'status')
```

**When to use**: Adding required fields to tables with existing data.

### Extension Setup (PostGIS, TimescaleDB)

Enable PostgreSQL extensions idempotently:

```python
def upgrade():
    # Enable extensions (safe to run multiple times)
    op.execute("CREATE EXTENSION IF NOT EXISTS postgis")
    op.execute("CREATE EXTENSION IF NOT EXISTS timescaledb")

    # Create hypertable for time-series data
    op.execute("""
        SELECT create_hypertable(
            'measurements',
            'recorded_at',
            chunk_time_interval => INTERVAL '1 month',
            if_not_exists => TRUE
        )
    """)

def downgrade():
    # Hypertables must be dropped before extension
    op.execute("DROP TABLE IF EXISTS measurements CASCADE")
    # Don't drop extensions (may be used by other tables)
```

**When to use**: Setting up spatial data (PostGIS) or time-series optimization (TimescaleDB).

### Index Creation with CONCURRENTLY

Create indexes without locking the table (critical for production):

```python
from alembic import op

def upgrade():
    # Create index without table lock (safe for production)
    # Note: Cannot be in a transaction, so we use op.execute directly
    op.execute(
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS "
        "idx_measurements_asset_time "
        "ON measurements (asset_uuid, recorded_at DESC)"
    )

def downgrade():
    op.execute("DROP INDEX CONCURRENTLY IF EXISTS idx_measurements_asset_time")
```

**When to use**: Adding indexes to large tables in production without downtime.

**Important**: `CREATE INDEX CONCURRENTLY` cannot run inside a transaction. You may need:

```python
from alembic import context

def upgrade():
    # Get connection for non-transactional DDL
    conn = context.get_bind()

    # If in transaction, we need to commit first
    if context.is_transactional_ddl():
        # Use regular CREATE INDEX (will lock table)
        op.execute(
            "CREATE INDEX IF NOT EXISTS idx_measurements_asset_time "
            "ON measurements (asset_uuid, recorded_at DESC)"
        )
    else:
        # Use CONCURRENTLY in production
        conn.execute(
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS "
            "idx_measurements_asset_time "
            "ON measurements (asset_uuid, recorded_at DESC)"
        )
```

### Renaming Columns Safely

Rename columns with backward compatibility:

```python
def upgrade():
    # Add new column with new name
    op.add_column('assets', sa.Column('asset_identifier', sa.String(100)))

    # Copy data from old column to new
    op.execute("UPDATE assets SET asset_identifier = old_identifier")

    # Make new column non-nullable
    op.alter_column('assets', 'asset_identifier', nullable=False)

    # Drop old column after data migration
    op.drop_column('assets', 'old_identifier')

def downgrade():
    # Reverse: add old column, copy data back, drop new column
    op.add_column('assets', sa.Column('old_identifier', sa.String(100)))
    op.execute("UPDATE assets SET old_identifier = asset_identifier")
    op.alter_column('assets', 'old_identifier', nullable=False)
    op.drop_column('assets', 'asset_identifier')
```

**When to use**: Renaming columns in production with zero downtime.

### Complex Multi-Table Migration

```python
def upgrade():
    # Create new table
    op.create_table(
        'alert_escalations',
        sa.Column('uuid', sa.UUID(), nullable=False),
        sa.Column('alert_uuid', sa.UUID(), nullable=False),
        sa.Column('level', sa.Integer(), nullable=False),
        sa.Column('escalated_at', sa.DateTime(timezone=True), nullable=False),
        sa.ForeignKeyConstraint(['alert_uuid'], ['alerts.uuid']),
        sa.PrimaryKeyConstraint('uuid')
    )

    # Create index
    op.create_index(
        'idx_alert_escalations_alert',
        'alert_escalations',
        ['alert_uuid', 'escalated_at']
    )

    # Add column to existing table
    op.add_column('alerts',
        sa.Column('escalation_level', sa.Integer(), nullable=True)
    )

def downgrade():
    op.drop_column('alerts', 'escalation_level')
    op.drop_index('idx_alert_escalations_alert')
    op.drop_table('alert_escalations')
```

**When to use**: Implementing new features requiring multiple related schema changes.

## CI/CD Integration

### GitHub Actions Workflow

```yaml
# .github/workflows/test.yml
name: Test

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest

    services:
      postgres:
        image: postgis/postgis:15-3.3
        env:
          POSTGRES_USER: corrdata
          POSTGRES_PASSWORD: corrdata
          POSTGRES_DB: corrdata_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install -e .
          pip install -r requirements-dev.txt

      - name: Run migrations
        env:
          DATABASE_URL: postgresql://corrdata:corrdata@localhost/corrdata_test
        run: |
          alembic upgrade head

      - name: Verify migration
        env:
          DATABASE_URL: postgresql://corrdata:corrdata@localhost/corrdata_test
        run: |
          alembic current
          # Ensure no pending migrations
          python -c "from alembic import command, config; cfg = config.Config('alembic.ini'); command.check(cfg)"

      - name: Run tests
        env:
          DATABASE_URL: postgresql://corrdata:corrdata@localhost/corrdata_test
        run: |
          pytest tests/ -v
```

### Deployment with Migrations

```yaml
# .github/workflows/deploy.yml
name: Deploy to Production

on:
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v3

      - name: Run migrations on production
        env:
          DATABASE_URL: ${{ secrets.PRODUCTION_DATABASE_URL }}
        run: |
          # Backup database first
          pg_dump $DATABASE_URL > backup_$(date +%Y%m%d_%H%M%S).sql

          # Run migrations
          alembic upgrade head

          # Verify
          alembic current
```

## CorrData Migration Examples

### Sprint 6: External Corrosion Factors

```python
# alembic/versions/e5b2c8a7f391_add_external_corrosion_factors_sprint_6.py
def upgrade():
    # Create external factors table
    op.create_table(
        'external_corrosion_factors',
        sa.Column('uuid', sa.UUID(), nullable=False),
        sa.Column('segment_uuid', sa.UUID(), nullable=False),
        sa.Column('soil_resistivity', sa.Float(), nullable=True),
        sa.Column('soil_ph', sa.Float(), nullable=True),
        sa.Column('soil_moisture', sa.Float(), nullable=True),
        sa.Column('bacteria_present', sa.Boolean(), nullable=True),
        sa.Column('stray_current_detected', sa.Boolean(), nullable=True),
        sa.ForeignKeyConstraint(['segment_uuid'], ['assets.uuid']),
        sa.PrimaryKeyConstraint('uuid')
    )

    # Create indexes for common queries
    op.create_index(
        'idx_external_factors_segment',
        'external_corrosion_factors',
        ['segment_uuid']
    )
```

### Sprint 7: Alerting System

```python
# alembic/versions/f7c3d9b8e412_add_alerting_system_sprint_7.py
def upgrade():
    # Create alerts table
    op.create_table(
        'alerts',
        sa.Column('uuid', sa.UUID(), nullable=False),
        sa.Column('rule_uuid', sa.UUID(), nullable=False),
        sa.Column('asset_uuid', sa.UUID(), nullable=False),
        sa.Column('priority', sa.String(20), nullable=False),
        sa.Column('status', sa.String(20), nullable=False),
        sa.Column('title', sa.String(255), nullable=False),
        sa.Column('message', sa.Text(), nullable=False),
        sa.Column('context', sa.JSON(), nullable=True),
        sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.func.now()),
        sa.Column('acknowledged_at', sa.DateTime(timezone=True), nullable=True),
        sa.ForeignKeyConstraint(['asset_uuid'], ['assets.uuid']),
        sa.PrimaryKeyConstraint('uuid')
    )

    # Index for deduplication queries
    op.create_index(
        'idx_alerts_rule_asset_created',
        'alerts',
        ['rule_uuid', 'asset_uuid', 'created_at']
    )
```

### Sprint 13: Climate Data (with TimescaleDB)

```python
# alembic/versions/g8b4c0d3e567_add_climate_weather_tables_sprint_13.py
def upgrade():
    # Enable TimescaleDB extension
    op.execute("CREATE EXTENSION IF NOT EXISTS timescaledb CASCADE")

    # Create climate measurements table
    op.create_table(
        'climate_measurements',
        sa.Column('uuid', sa.UUID(), nullable=False),
        sa.Column('segment_uuid', sa.UUID(), nullable=False),
        sa.Column('recorded_at', sa.DateTime(timezone=True), nullable=False),
        sa.Column('temperature', sa.Float(), nullable=True),
        sa.Column('precipitation', sa.Float(), nullable=True),
        sa.Column('humidity', sa.Float(), nullable=True),
        sa.ForeignKeyConstraint(['segment_uuid'], ['assets.uuid']),
        sa.PrimaryKeyConstraint('uuid', 'recorded_at')
    )

    # Convert to hypertable for time-series optimization
    op.execute("""
        SELECT create_hypertable(
            'climate_measurements',
            'recorded_at',
            chunk_time_interval => INTERVAL '1 month',
            if_not_exists => TRUE
        )
    """)
```

## Verification

### Verify Migration Status

```bash
# Check current migration
alembic current

# Expected output:
# abc123def456 (head)

# Show migration history
alembic history

# Check for pending migrations
alembic check
# No output = all migrations applied
```

### Verify Schema Changes

```bash
# Inspect table schema
psql $DATABASE_URL -c "\d assets"

# Expected output shows new columns:
#   coating_type          | character varying(50)
#   installed_at          | timestamp with time zone
```

### Verify Data Migration

```sql
-- If you migrated data, verify it worked
SELECT COUNT(*) FROM assets WHERE status IS NULL;
-- Should be 0 if data migration set all status values

SELECT COUNT(*) FROM assets WHERE status = 'active';
-- Should show migrated count
```

## Learnings

### From Sprint 6-116 (Multiple Migration Sprints)
- Always test migrations on staging before production
- Use descriptive migration messages that reference sprint numbers
- Keep migrations small and focused (one logical change per migration)
- Test both upgrade AND downgrade paths
- Document complex data migrations with comments
- Use `IF NOT EXISTS` for idempotent operations (extensions, indexes)
- Add indexes CONCURRENTLY in production to avoid table locks

### Migration Naming Convention
CorrData uses descriptive names with sprint context:
```
{auto_revision_id}_{description}_sprint_{number}.py

Examples:
- e5b2c8a7f391_add_external_corrosion_factors_sprint_6.py
- f7c3d9b8e412_add_alerting_system_sprint_7.py
- sprint103_compliance_graphql.py
```

### Common Gotchas
- Alembic doesn't detect: Renamed columns, renamed tables, changes to server defaults, custom constraints
- TimescaleDB hypertables: Must drop before dropping extension
- Foreign keys: Must drop dependent tables before parent
- Indexes on large tables: Use CONCURRENTLY in production
- Enum types: Must be created/dropped explicitly, Alembic doesn't auto-detect
- JSON columns: Data structure changes aren't detected by autogenerate

### Performance Considerations
- Large table alterations: Consider maintenance window
- Adding indexes: Use CONCURRENTLY to avoid locks
- Populating columns: Batch updates for large tables
- Dropping columns: PostgreSQL doesn't reclaim space immediately (VACUUM FULL needed)

## Anti-Patterns

### Don't: Edit Existing Migrations

**What it looks like**: Changing a migration file that's already been applied to production.

**Why it's bad**: Other environments have already applied the old version. Editing creates inconsistent state.

**Instead**: Create a new migration to make additional changes.

### Don't: Skip Testing Downgrades

**What it looks like**: Only testing `alembic upgrade head`, never testing `alembic downgrade -1`.

**Why it's bad**: Production rollbacks fail when downgrades don't work.

**Instead**: Always test downgrade locally before deploying migration.

### Don't: Use Non-Idempotent Operations

**What it looks like**:
```python
op.execute("CREATE EXTENSION postgis")  # Fails if already exists
```

**Why it's bad**: Migration fails if run twice (e.g., partial failure, retry).

**Instead**:
```python
op.execute("CREATE EXTENSION IF NOT EXISTS postgis")
```

### Don't: Delete Data in Downgrade

**What it looks like**:
```python
def downgrade():
    op.drop_column('assets', 'important_data')  # Data lost!
```

**Why it's bad**: Downgrade is data-destructive, can't recover.

**Instead**: Document that downgrade is destructive, or preserve data:
```python
def downgrade():
    # WARNING: This will delete all coating_type data
    # Consider backing up first:
    # SELECT uuid, coating_type INTO coating_type_backup FROM assets;
    op.drop_column('assets', 'coating_type')
```

### Don't: Ignore Autogenerate Warnings

**What it looks like**: Alembic says "Detected added table 'xyz'" but you didn't add a table.

**Why it's bad**: Autogenerate detected something unexpected, might be incorrect.

**Instead**: Investigate warnings. Common causes:
- Imports missing from `alembic/env.py`
- Models defined but not imported
- External tables detected (use `include_object` filter)

## Variations

### For High-Traffic Production Tables

When altering large, critical tables:

```python
def upgrade():
    # 1. Create new table with desired schema
    op.create_table(
        'assets_new',
        sa.Column('uuid', sa.UUID(), nullable=False),
        sa.Column('name', sa.String(255), nullable=False),
        sa.Column('coating_type', sa.String(50), nullable=True),  # New column
        sa.PrimaryKeyConstraint('uuid')
    )

    # 2. Copy data incrementally (batches)
    op.execute("""
        INSERT INTO assets_new (uuid, name, coating_type)
        SELECT uuid, name, NULL
        FROM assets
    """)

    # 3. Swap tables (atomic)
    op.rename_table('assets', 'assets_old')
    op.rename_table('assets_new', 'assets')

    # 4. Drop old table after verification
    # op.drop_table('assets_old')  # Do this in next migration after verification
```

### For Multi-Database Migrations

If you need to migrate multiple databases (primary + replica):

```python
# alembic.ini - define multiple sections
[alembic:primary]
sqlalchemy.url = postgresql://...primary

[alembic:replica]
sqlalchemy.url = postgresql://...replica

# Run migrations on both
alembic -n primary upgrade head
alembic -n replica upgrade head
```

## Related Recipes

- [Configuration Management](../patterns/configuration-management.md) - Database URL configuration
- [Alert Engine Configuration](../patterns/alert-engine.md) - Alert schema migrations

## References

- **Alembic Directory**: `/Volumes/Foundry/Development/CorrData/POC/alembic/`
  - `env.py` - Environment configuration, model imports
  - `versions/` - All migration files
- **Database Models**: `/Volumes/Foundry/Development/CorrData/POC/src/corrdata/db/`
  - `models.py` - Core asset models
  - `alert_models.py` - Alert system models
  - `external_models.py` - External corrosion factor models
  - `climate_models.py` - Climate/weather models
- **ADRs**:
  - [ADR-010: Alembic Database Migrations](../../architecture/decisions/ADR-010-alembic-database-migrations.md)
  - [ADR-011: SQLAlchemy 2.0 ORM](../../architecture/decisions/ADR-011-sqlalchemy-2-orm.md)
  - [ADR-007: TimescaleDB for Time-Series](../../architecture/decisions/ADR-007-timescaledb-time-series.md)
  - [ADR-008: PostGIS for Spatial Data](../../architecture/decisions/ADR-008-postgis-spatial-data.md)

## Changelog

| Version | Date | Changes |
|---------|------|---------|
| 1.0 | 2025-12-14 | Initial version based on ADR-010 and Sprint 6-116 migrations |
